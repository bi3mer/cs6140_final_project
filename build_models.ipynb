{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "Using TensorFlow backend.\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = pd.read_csv('vids.csv')\n",
    "X = vids[vids.category_id.notna()]\n",
    "X = X.drop(([\n",
    "     'video_id', 'channel_title', 'trending_date', 'publish_time', 'comments_disabled',\n",
    "     'comment_count', 'likes', 'dislikes', 'video_error_or_removed', 'thumbnail_link',\n",
    "     'ratings_disabled']), 1)\n",
    "Y = vids.views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy encode categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cateogries = pd.get_dummies(X.category_id, prefix=\"category_id\")\n",
    "X = X.drop(['category_id'], 1)\n",
    "for column in encoded_cateogries.columns:\n",
    "    X[column] = encoded_cateogries[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode words with w2v model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd5599efdf243cbac28446ae68f9603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41037.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_titles = []\n",
    "processed_descriptions = []\n",
    "processed_tags = []\n",
    "\n",
    "for i in tqdm(range(len(X))):\n",
    "    try:\n",
    "        processed_titles.append(simple_preprocess(X.title.iloc[i]))\n",
    "    except:\n",
    "        processed_titles.append([\"\"])\n",
    "\n",
    "    try:\n",
    "        processed_descriptions.append(simple_preprocess(X.description.iloc[i]))\n",
    "    except:\n",
    "        processed_descriptions.append([\"\"])\n",
    "\n",
    "    try:\n",
    "        processed_tags.append(simple_preprocess(X.tags.iloc[i]))\n",
    "    except:\n",
    "        processed_tags.append([\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4d133db4f7423db2585ab968064be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41037.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(os.path.join('mdl', 'word2vec.model'))\n",
    "all_one_feature = []\n",
    "dscription_feature = []\n",
    "title_feature = []\n",
    "tags_feature = []\n",
    "\n",
    "for i in tqdm(range(len(X))):\n",
    "    filtered = [word for word in processed_titles[i] if word in w2v_model.wv.vocab]\n",
    "    if len(filtered) > 0:\n",
    "        title_vec = np.mean(w2v_model.wv[filtered], axis=0)\n",
    "    else:\n",
    "        title_vec = np.zeros(w2v_model.vector_size)\n",
    "        \n",
    "    filtered = [word for word in processed_descriptions[i] if word in w2v_model.wv.vocab]\n",
    "    if len(filtered) > 0:\n",
    "        description_vec = np.mean(w2v_model.wv[filtered], axis=0)\n",
    "    else:\n",
    "        description_vec = np.zeros(w2v_model.vector_size)\n",
    "        \n",
    "    filtered = [word for word in processed_tags[i] if word in w2v_model.wv.vocab]\n",
    "    if len(filtered) > 0:\n",
    "        tags_vec = np.mean(w2v_model.wv[filtered], axis=0)\n",
    "    else:\n",
    "        tags_vec = np.zeros(w2v_model.vector_size)\n",
    "    \n",
    "    all_one_feature.append(np.mean([title_vec, description_vec, tags_vec], axis=0))\n",
    "    dscription_feature.append(description_vec)\n",
    "    title_feature.append(title_vec)\n",
    "    tags_feature.append(tags_vec)\n",
    "\n",
    "all_one_feature = np.array(all_one_feature)\n",
    "dscription_feature = np.array(dscription_feature)\n",
    "title_feature = np.array(title_feature)\n",
    "tags_feature = np.array(tags_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20096433c9c748598738131a12e7a79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = X.drop(['title', 'tags', 'description'], 1)\n",
    "\n",
    "for i in tqdm(range(w2v_model.vector_size)):\n",
    "    X[f'word_encodings{i}'] = all_one_feature[:,i]\n",
    "    X[f'description{i}'] = dscription_feature[:, i]\n",
    "    X[f'title{i}'] = title_feature[:,i]\n",
    "    X[f'tags{i}'] = tags_feature[:,i]\n",
    "\n",
    "X = X.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(X, test_size=0.2)\n",
    "\n",
    "train_y = train.views\n",
    "train_x = train.drop('views', 1)\n",
    "test_y = test.views\n",
    "test_x = test.drop('views', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = np.array([[f'title{i}', f'tags{i}', f'description{i}'] for i in range(w2v_model.vector_size)]).flatten()\n",
    "train_x_all = train_x.drop(to_drop,1)\n",
    "test_x_all = test_x.drop(to_drop,1)\n",
    "\n",
    "to_drop = [f'word_encodings{i}' for i in range(w2v_model.vector_size)]\n",
    "train_x_sep = train_x.drop(to_drop,1)\n",
    "test_x_sep = test_x.drop(to_drop,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store Train/Test Split**\n",
    "\n",
    "We may use it for some future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('data')):\n",
    "    os.mkdir(os.path.join('data'))\n",
    "    \n",
    "train_x_all.to_csv(os.path.join('data','train_x_all.csv'), index=False)\n",
    "train_x_sep.to_csv(os.path.join('data','train_x_sep.csv'), index=False)\n",
    "\n",
    "test_x_all.to_csv(os.path.join('data','test_x_all.csv'), index=False)\n",
    "test_x_sep.to_csv(os.path.join('data','test_x_sep.csv'), index=False)\n",
    "\n",
    "train_y.to_csv(os.path.join('data','train_y.csv'), index=False)\n",
    "test_y.to_csv(os.path.join('data','test_y.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_network(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_layer_network(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim, input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(int(input_dim/4), input_dim=input_dim, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(int(input_dim/8), input_dim=int(input_dim/4), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(int(input_dim/16), input_dim=int(input_dim/8), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(int(input_dim/32), input_dim=int(input_dim/16), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "lr_all = LinearRegression()\n",
    "lr_sep = LinearRegression()\n",
    "\n",
    "svr_all = LinearSVR(C=1.0, epsilon=0.2)\n",
    "svr_sep = LinearSVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "rfr_all = RandomForestRegressor(n_estimators=20, max_depth=3)\n",
    "rfr_sep = RandomForestRegressor(n_estimators=20, max_depth=3)\n",
    "\n",
    "xgb_all = XGBRegressor(n_estimators=300)\n",
    "xgb_sep = XGBRegressor(n_estimators=300)\n",
    "\n",
    "gbm_all = lgbm.LGBMRegressor()\n",
    "gbm_sep = lgbm.LGBMRegressor()\n",
    "\n",
    "nn1_all = single_layer_network(train_x_all.shape[1])\n",
    "nn1_sep = single_layer_network(train_x_sep.shape[1])\n",
    "\n",
    "nn5_all = five_layer_network(train_x_all.shape[1])\n",
    "nn5_sep = five_layer_network(train_x_sep.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lr_all\n",
      "training lr_sep\n",
      "training svr_all\n",
      "training svr_sep\n",
      "training rfr_all\n",
      "training rfr_sep\n",
      "training xgb_all\n",
      "training xgb_sep\n",
      "training lgbm_all\n",
      "training lgbm_sep\n",
      "training nn1_all\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "32688/32688 [==============================] - 4s 114us/step - loss: 59278540097741.5000\n",
      "Epoch 2/10\n",
      "32688/32688 [==============================] - 4s 113us/step - loss: 59096079208105.1641\n",
      "Epoch 3/10\n",
      "32688/32688 [==============================] - 4s 114us/step - loss: 58779506208324.9141\n",
      "Epoch 4/10\n",
      "32688/32688 [==============================] - 4s 117us/step - loss: 58378161851896.7344\n",
      "Epoch 5/10\n",
      "32688/32688 [==============================] - 3s 107us/step - loss: 57921594907465.0547\n",
      "Epoch 6/10\n",
      "32688/32688 [==============================] - 4s 111us/step - loss: 57438152922878.3750\n",
      "Epoch 7/10\n",
      "32688/32688 [==============================] - 4s 111us/step - loss: 56948119701701.4766\n",
      "Epoch 8/10\n",
      "32688/32688 [==============================] - 3s 105us/step - loss: 56474454366239.0781\n",
      "Epoch 9/10\n",
      "32688/32688 [==============================] - 4s 112us/step - loss: 56029930403137.2812\n",
      "Epoch 10/10\n",
      "32688/32688 [==============================] - 4s 113us/step - loss: 55621441940403.3125\n",
      "training nn1_sep\n",
      "Epoch 1/10\n",
      "32688/32688 [==============================] - 7s 225us/step - loss: 59194850225660.7422\n",
      "Epoch 2/10\n",
      "32688/32688 [==============================] - 7s 208us/step - loss: 58563840853740.3281\n",
      "Epoch 3/10\n",
      "32688/32688 [==============================] - 7s 214us/step - loss: 57563509159610.2109\n",
      "Epoch 4/10\n",
      "32688/32688 [==============================] - 7s 207us/step - loss: 56509215455186.8906\n",
      "Epoch 5/10\n",
      "32688/32688 [==============================] - 7s 213us/step - loss: 55550741566885.5234\n",
      "Epoch 6/10\n",
      "32688/32688 [==============================] - 7s 210us/step - loss: 54778219805829.8203\n",
      "Epoch 7/10\n",
      "32688/32688 [==============================] - 7s 205us/step - loss: 54226461453010.7656\n",
      "Epoch 8/10\n",
      "32688/32688 [==============================] - 7s 205us/step - loss: 53846650773676.4219\n",
      "Epoch 9/10\n",
      "32688/32688 [==============================] - 7s 208us/step - loss: 53582298923641.5469\n",
      "Epoch 10/10\n",
      "32688/32688 [==============================] - 7s 217us/step - loss: 53382005379041.9219\n",
      "training nn5_all\n",
      "Epoch 1/10\n",
      "32688/32688 [==============================] - 6s 185us/step - loss: 55881454662014.9375\n",
      "Epoch 2/10\n",
      "32688/32688 [==============================] - 5s 165us/step - loss: 53434322973271.4609\n",
      "Epoch 3/10\n",
      "32688/32688 [==============================] - 6s 173us/step - loss: 52265946135300.8828\n",
      "Epoch 4/10\n",
      "32688/32688 [==============================] - 6s 170us/step - loss: 49675411032469.9844\n",
      "Epoch 5/10\n",
      "32688/32688 [==============================] - 5s 164us/step - loss: 47598852227352.6875\n",
      "Epoch 6/10\n",
      "32688/32688 [==============================] - 5s 168us/step - loss: 46668414219721.6172\n",
      "Epoch 7/10\n",
      "32688/32688 [==============================] - 5s 160us/step - loss: 45961205576215.3047\n",
      "Epoch 8/10\n",
      "32688/32688 [==============================] - 5s 166us/step - loss: 45316668854121.6328\n",
      "Epoch 9/10\n",
      "32688/32688 [==============================] - 5s 161us/step - loss: 44798398996323.6172\n",
      "Epoch 10/10\n",
      "32688/32688 [==============================] - 6s 172us/step - loss: 44254669507687.7500\n",
      "training nn5_sep\n",
      "Epoch 1/10\n",
      "32688/32688 [==============================] - 10s 309us/step - loss: 53029303271865.0781\n",
      "Epoch 2/10\n",
      "32688/32688 [==============================] - 10s 293us/step - loss: 45894108123498.8906\n",
      "Epoch 3/10\n",
      "32688/32688 [==============================] - 10s 294us/step - loss: 41624762826583.5859\n",
      "Epoch 4/10\n",
      "32688/32688 [==============================] - 10s 294us/step - loss: 36825386251416.8750\n",
      "Epoch 5/10\n",
      "32688/32688 [==============================] - 9s 287us/step - loss: 33057642251074.0352\n",
      "Epoch 6/10\n",
      "32688/32688 [==============================] - 10s 292us/step - loss: 30208809781678.0508\n",
      "Epoch 7/10\n",
      "32688/32688 [==============================] - 9s 285us/step - loss: 28010854181410.3320\n",
      "Epoch 8/10\n",
      "32688/32688 [==============================] - 10s 291us/step - loss: 26126593942702.9258\n",
      "Epoch 9/10\n",
      "32688/32688 [==============================] - 9s 289us/step - loss: 24375989847884.5625\n",
      "Epoch 10/10\n",
      "32688/32688 [==============================] - 9s 287us/step - loss: 22959093531850.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8d45703a90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('training lr_all')\n",
    "lr_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training lr_sep')\n",
    "lr_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training svr_all')\n",
    "svr_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training svr_sep')\n",
    "svr_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training rfr_all')\n",
    "rfr_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training rfr_sep')\n",
    "rfr_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training xgb_all')\n",
    "xgb_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training xgb_sep')\n",
    "xgb_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training lgbm_all')\n",
    "gbm_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training lgbm_sep')\n",
    "gbm_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training nn1_all')\n",
    "nn1_all.fit(train_x_all, train_y, epochs=10)\n",
    "\n",
    "print('training nn1_sep')\n",
    "nn1_sep.fit(train_x_sep, train_y, epochs=10)\n",
    "\n",
    "print('training nn5_all')\n",
    "nn5_all.fit(train_x_all, train_y, epochs=10)\n",
    "\n",
    "print('training nn5_sep')\n",
    "nn5_sep.fit(train_x_sep, train_y, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr_all, open(os.path.join('mdl', 'lr_all.pkl'), 'wb'))\n",
    "pickle.dump(lr_sep, open(os.path.join('mdl', 'lr_sep.pkl'), 'wb'))\n",
    "pickle.dump(svr_all, open(os.path.join('mdl', 'svr_all.pkl'), 'wb'))\n",
    "pickle.dump(svr_sep, open(os.path.join('mdl', 'svr_sep.pkl'), 'wb'))\n",
    "pickle.dump(rfr_all, open(os.path.join('mdl', 'rfr_all.pkl'), 'wb'))\n",
    "pickle.dump(rfr_sep, open(os.path.join('mdl', 'rfr_sep.pkl'), 'wb'))\n",
    "pickle.dump(xgb_all, open(os.path.join('mdl', 'xgb_all.pkl'), 'wb'))\n",
    "pickle.dump(xgb_sep, open(os.path.join('mdl', 'xgb_sep.pkl'), 'wb'))\n",
    "pickle.dump(gbm_all, open(os.path.join('mdl', 'gbm_all.pkl'), 'wb'))\n",
    "pickle.dump(gbm_sep, open(os.path.join('mdl', 'gbm_sep.pkl'), 'wb'))\n",
    "pickle.dump(nn1_all, open(os.path.join('mdl', 'nn1_all.pkl'), 'wb'))\n",
    "pickle.dump(nn1_sep, open(os.path.join('mdl', 'nn1_sep.pkl'), 'wb'))\n",
    "pickle.dump(nn5_all, open(os.path.join('mdl', 'nn5_all.pkl'), 'wb'))\n",
    "pickle.dump(nn5_sep, open(os.path.join('mdl', 'nn5_sep.pkl'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
