{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:14: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vids = pd.read_csv('../vids.csv')\n",
    "X = vids[vids.category_id.notna()]\n",
    "X = X.drop(([\n",
    "     'video_id', 'channel_title', 'trending_date', 'publish_time', 'comments_disabled',\n",
    "     'comment_count', 'likes', 'dislikes', 'video_error_or_removed', 'thumbnail_link',\n",
    "     'ratings_disabled']), 1)\n",
    "Y = vids.views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy encode categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cateogries = pd.get_dummies(X.category_id, prefix=\"category_id\")\n",
    "X = X.drop(['category_id'], 1)\n",
    "for column in encoded_cateogries.columns:\n",
    "    X[column] = encoded_cateogries[column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode words with w2v model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77c369952d143868b3e28308a740821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41037.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "processed_titles = []\n",
    "processed_descriptions = []\n",
    "processed_tags = []\n",
    "\n",
    "for i in tqdm(range(len(X))):\n",
    "    try:\n",
    "        processed_titles.append(simple_preprocess(X.title.iloc[i]))\n",
    "    except:\n",
    "        processed_titles.append([\"\"])\n",
    "\n",
    "    try:\n",
    "        processed_descriptions.append(simple_preprocess(X.description.iloc[i]))\n",
    "    except:\n",
    "        processed_descriptions.append([\"\"])\n",
    "\n",
    "    try:\n",
    "        processed_tags.append(simple_preprocess(X.tags.iloc[i]))\n",
    "    except:\n",
    "        processed_tags.append([\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa51dfae4304511bf8d4753e3e4a5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=41037.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(os.path.join('..', 'mdl', 'word2vec.model'))\n",
    "all_one_feature = []\n",
    "dscription_feature = []\n",
    "title_feature = []\n",
    "tags_feature = []\n",
    "\n",
    "for i in tqdm(range(len(X))):\n",
    "    filtered = [word for word in processed_titles[i] if word in w2v_model.wv.vocab]\n",
    "    if len(filtered) > 0:\n",
    "        title_vec = np.mean(w2v_model.wv[filtered], axis=0)\n",
    "    else:\n",
    "        title_vec = np.zeros(w2v_model.vector_size)\n",
    "        \n",
    "    filtered = [word for word in processed_descriptions[i] if word in w2v_model.wv.vocab]\n",
    "    if len(filtered) > 0:\n",
    "        description_vec = np.mean(w2v_model.wv[filtered], axis=0)\n",
    "    else:\n",
    "        description_vec = np.zeros(w2v_model.vector_size)\n",
    "        \n",
    "    filtered = [word for word in processed_tags[i] if word in w2v_model.wv.vocab]\n",
    "    if len(filtered) > 0:\n",
    "        tags_vec = np.mean(w2v_model.wv[filtered], axis=0)\n",
    "    else:\n",
    "        tags_vec = np.zeros(w2v_model.vector_size)\n",
    "    \n",
    "    all_one_feature.append(np.mean([title_vec, description_vec, tags_vec], axis=0))\n",
    "    dscription_feature.append(description_vec)\n",
    "    title_feature.append(title_vec)\n",
    "    tags_feature.append(tags_vec)\n",
    "\n",
    "all_one_feature = np.array(all_one_feature)\n",
    "dscription_feature = np.array(dscription_feature)\n",
    "title_feature = np.array(title_feature)\n",
    "tags_feature = np.array(tags_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0562507da24b35b3fc0211e5e1d942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X = X.drop(['title', 'tags', 'description'], 1)\n",
    "\n",
    "for i in tqdm(range(w2v_model.vector_size)):\n",
    "    X[f'word_encodings{i}'] = all_one_feature[:,i]\n",
    "    X[f'description{i}'] = dscription_feature[:, i]\n",
    "    X[f'title{i}'] = title_feature[:,i]\n",
    "    X[f'tags{i}'] = tags_feature[:,i]\n",
    "\n",
    "X = X.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(X, test_size=0.2)\n",
    "\n",
    "train_y = train.views\n",
    "train_x = train.drop('views', 1)\n",
    "test_y = test.views\n",
    "test_x = test.drop('views', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = np.array([[f'title{i}', f'tags{i}', f'description{i}'] for i in range(w2v_model.vector_size)]).flatten()\n",
    "train_x_all = train_x.drop(to_drop,1)\n",
    "test_x_all = test_x.drop(to_drop,1)\n",
    "\n",
    "to_drop = [f'word_encodings{i}' for i in range(w2v_model.vector_size)]\n",
    "train_x_sep = train_x.drop(to_drop,1)\n",
    "test_x_sep = test_x.drop(to_drop,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store Train/Test Split**\n",
    "\n",
    "We may use it for some future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('..', 'data')):\n",
    "    os.mkdir(os.path.join('..', 'data'))\n",
    "    \n",
    "train_x_all.to_csv(os.path.join('..', 'data','train_x_all.csv'), index=False)\n",
    "train_x_sep.to_csv(os.path.join('..', 'data','train_x_sep.csv'), index=False)\n",
    "test_x_all.to_csv(os.path.join('..', 'data','test_x_all.csv'), index=False)\n",
    "test_x_sep.to_csv(os.path.join('..', 'data','test_x_sep.csv'), index=False)\n",
    "train_y.to_csv(os.path.join('..', 'data','train_y.csv'), index=False)\n",
    "test_y.to_csv(os.path.join('..', 'data','test_y.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_all = LinearRegression()\n",
    "lr_sep = LinearRegression()\n",
    "\n",
    "svr_all = LinearSVR(C=1.0, epsilon=0.2)\n",
    "svr_sep = LinearSVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "rfr_all = RandomForestRegressor(n_estimators=20, max_depth=3)\n",
    "rfr_sep = RandomForestRegressor(n_estimators=20, max_depth=3)\n",
    "\n",
    "xgb_all = XGBRegressor(n_estimators=300)\n",
    "xgb_sep = XGBRegressor(n_estimators=300)\n",
    "\n",
    "gbm_all = lgbm.LGBMRegressor()\n",
    "gbm_sep = lgbm.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lr_all\n",
      "training lr_sep\n",
      "training svr_all\n",
      "training svr_sep\n",
      "training rfr_all\n",
      "training rfr_sep\n",
      "training xgb_all\n",
      "training xgb_sep\n",
      "training lgbm_all\n",
      "training lgbm_sep\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('training lr_all')\n",
    "lr_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training lr_sep')\n",
    "lr_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training svr_all')\n",
    "svr_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training svr_sep')\n",
    "svr_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training rfr_all')\n",
    "rfr_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training rfr_sep')\n",
    "rfr_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training xgb_all')\n",
    "xgb_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training xgb_sep')\n",
    "xgb_sep.fit(train_x_sep, train_y)\n",
    "\n",
    "print('training lgbm_all')\n",
    "gbm_all.fit(train_x_all, train_y)\n",
    "\n",
    "print('training lgbm_sep')\n",
    "gbm_sep.fit(train_x_sep, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = os.path.join('..', 'mdl')\n",
    "pickle.dump(lr_all, open(os.path.join(mdl_path, 'lr_all.pkl'), 'wb'))\n",
    "pickle.dump(lr_sep, open(os.path.join(mdl_path, 'lr_sep.pkl'), 'wb'))\n",
    "pickle.dump(svr_all, open(os.path.join(mdl_path, 'svr_all.pkl'), 'wb'))\n",
    "pickle.dump(svr_sep, open(os.path.join(mdl_path, 'svr_sep.pkl'), 'wb'))\n",
    "pickle.dump(rfr_all, open(os.path.join(mdl_path, 'rfr_all.pkl'), 'wb'))\n",
    "pickle.dump(rfr_sep, open(os.path.join(mdl_path, 'rfr_sep.pkl'), 'wb'))\n",
    "pickle.dump(xgb_all, open(os.path.join(mdl_path, 'xgb_all.pkl'), 'wb'))\n",
    "pickle.dump(xgb_sep, open(os.path.join(mdl_path, 'xgb_sep.pkl'), 'wb'))\n",
    "pickle.dump(gbm_all, open(os.path.join(mdl_path, 'gbm_all.pkl'), 'wb'))\n",
    "pickle.dump(gbm_sep, open(os.path.join(mdl_path, 'gbm_sep.pkl'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
